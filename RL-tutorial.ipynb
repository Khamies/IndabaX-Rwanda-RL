{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this practical we introduce the idea of reinforcement learning, discuss how it differs from supervised and unsupervised learning and then build an agent that learns to drive a car up the mountain.\n",
    "\n",
    "# Learning Objectives\n",
    "* Understand the relationship between the **environment** and the  **agent**  \n",
    "* Understand how a **policy** is used by an agent to select an action\n",
    "* Describe how to implement a **run-loop** that controls the interaction between environement and agent.\n",
    "* Understand how the **state**, **action** and **reward** are communicated between the agent and the environment.  \n",
    "* Be able to implement the a simple **Q-learning** RL algorithm call **DQN**\n",
    "* Discover at least one potential issue with the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # A) Learning Paradims\n",
    "**Supervised learning: ** is a learning paradaim, given an input and a target value or class. The goal is to predict the class value.\n",
    "\n",
    "**Unsupervised learning: ** is a learning paradaim, where we are only given an input. The goal is to look for patterns in that input. \n",
    "\n",
    "**Reinforcement learning: ** which cares about training an **agent** to maximise a **reward** it obtains through interaction with an **environment**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Reinforcement Learning ( RL )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How RL works?\n",
    "\n",
    "The **environment** defines a set of **actions** that an agent can take. The agent observes the current **state** of the environment, tries actions and *learns* a **policy** which is a distribution over the possible actions given a state of the environment. \n",
    "\n",
    "The following diagram illustrates the interaction between the agent and environment. We will explore each of the terms in more detail throughout this practical. \n",
    "\n",
    "\n",
    "![RL Overview](data/rl-image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Outlines\n",
    "\n",
    "We will use an OpenAI Gym environment called **Mountain Car**, the goal is to train an agent to drive a car up the mountain. The practical will be as follows:\n",
    "\n",
    "1. Introduce the **Mountain Car** environment and explore the states and actions available.\n",
    "2. Create a simple agent that takes random actions.\n",
    "3. Going from random agent to skilled agent:\n",
    "    - Introduce Q-learning algorithm.\n",
    "    - The intuition behind Q-learning.\n",
    "    - Ways to implement Q-learning:\n",
    "        - Tabular Method\n",
    "        - Function approximator: Neural Network.\n",
    "        \n",
    "4. Merge Q-learning with Deep Learning:\n",
    "    - Introduce DQN algorithm.\n",
    "    - Play with DQN!!\n",
    "  \n",
    "5. Dive deeper into DQN:\n",
    "    - DQN main momponents\n",
    "    - Implement the components.\n",
    "    - Stack the components together.\n",
    "    \n",
    "6. The big picture of Reinforcement learing:\n",
    "    - General RL taxonomy\n",
    "        - Value function based method.\n",
    "        - Gradient based Methods.\n",
    "        - Hybrid based Methods.\n",
    "    - Drawback of the Value function Methods.\n",
    "    - Next Steps.\n",
    "\n",
    "7. References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Explore the Environment\n",
    "\n",
    "![SegmentLocal](data/environment.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Now let us take a look to how the states and actions represented:\n",
    "\n",
    "**States:**\n",
    "\n",
    "\n",
    "|Representation |  State |   Min value|  Max value  |\n",
    "|---|---------------|------|-------|\n",
    "| 0 |  position |  -1.2 |     0.6 | \n",
    "| 1 |   velocity|   -0.07|     0.07 | \n",
    "\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "\n",
    "|Representation |  Action| \n",
    "|---|---------------|\n",
    "| 0 |  push left |\n",
    "| 1 |   no push|  \n",
    "| 2 |    push right|  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car = gym.make('MountainCar-v0')\n",
    "# mountain_car = gym.make('CartPole-v0')\n",
    "\n",
    "num_states = mountain_car.observation_space.shape[0]\n",
    "num_actions = mountain_car.action_space.n\n",
    "\n",
    "print(\"States:  {} Type: Contiuous  Represented as: Vector \".format(num_states,) )\n",
    "print(\"Actions: {} Type: Discrete   Represented as: scalar or number \".format(num_actions+1))  # adding one, because action are starting from 0.\n",
    "print(\"Example of a state: \", mountain_car.observation_space.sample())\n",
    "print(\"Example of an action: \", mountain_car.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Build a random agent\n",
    "\n",
    "**reset:** get an initial state in the envirnoment.\n",
    "\n",
    "**render:** show the mountain car environemnt (simulator) in the screen.\n",
    "\n",
    "**step:** apply the action in the environemnt.\n",
    "\n",
    "**smaple:** sample an action\n",
    "\n",
    "**close:**  close the envrinment ( simulator ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car.reset()\n",
    "for _ in range(1000):\n",
    "    mountain_car.render()\n",
    "    random_agent = mountain_car.action_space.sample()\n",
    "    mountain_car.step(random_agent) # take a random action\n",
    "mountain_car.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Going from random agent to skilled agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Q-learning\n",
    "\n",
    "As we mentioned before, the goal of Reinforcement learning is to train an agent to maximise a reward it obtains through interaction with an environment.\n",
    "\n",
    "So, what is reward that the agent want to maximize?\n",
    "\n",
    "Basically, it is the objective function of the agent, and has another name called Reward function. It formalized matmatically as:\n",
    "\n",
    "$ G_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\gamma^{3} r_{t+3} + .... =  \\sum_{t=1}^{T_i} \\gamma^t r_{i,t}$\n",
    "\n",
    "Q-learning considers that every action in a given state has a value $ Q(s_t,a_t)$, this value formalized matmatically as:\n",
    "\n",
    "\n",
    "$ Q(s_t,a_t) = Q(s_t,a_t) + \\alpha [r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] $\n",
    "\n",
    "which is equal to : $ E[G_t|\\space s_t ,a_t]$\n",
    "\n",
    "Looking for a proof for the previous formula? go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  The intuition behind Q-learning.\n",
    "\n",
    "Now, Let us get the intiution behind this formula:\n",
    "\n",
    "\n",
    "$ Q(s_t,a_t) = Q(s_t,a_t) + \\alpha [r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] $\n",
    "\n",
    "$Q(s_t,a_t) \\equiv Action \\space value \\space a_t \\space at \\space state \\space s_t \\\\\n",
    "\\max_{a} \\gamma Q(s_{t+1},a) \\equiv max \\space action \\space value \\space over \\space all \\space the \\space actions \\space in \\space the \\space next \\space state \\space s_{t+1} \\equiv is \\space called \\space TD \\space target \\\\ \n",
    "[r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] \\equiv is \\space called \\space TD \\space error.\n",
    "$\n",
    "\n",
    "The formula actually saying: the value of the action depends on its previous value plus the discount rewards since taking that action and onward.\n",
    "This the discount rewards representes TD target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ways to implement Q-learning\n",
    "\n",
    "1. You can implement Q-learning using:\n",
    "\n",
    "    - Q-Table ( Dictionary, Matrix ,.... ) : Each cell in the table represents action value in a given state.\n",
    "    - Function Approximation: Because of the increasing of the Q-table with number of states. approximate the action value.\n",
    "             \n",
    "|   |  |  Q-Table |    |\n",
    "|---|---------------|------|-------|---\n",
    "| $s_0$ |  $a_{00}$|  $a_{01}$|     $a_{02}$ | $a_{03}$ | ... |...$a_{0n}$...\n",
    "| $s_1$ |    $a_{10}$|  $a_{11}$|     $a_{12}$ | $a_{13}$ | ... |...$a_{1n}$...\n",
    "| $s_2$ |   $a_{20}$|  $a_{21}$|     $a_{22}$ | $a_{23}$ | ... |...$a_{2n}$...\n",
    "| $s_3$|   $a_{30}$|  $a_{31}$|     $a_{32}$ | $a_{33}$ | ... |...$a_{3n}$...\n",
    "| .. |  ..|  ..|      .. | .. | ... |...\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Merge Q-learning with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DQN import Agent\n",
    "from DQN import run\n",
    "from DQN import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent\n",
    "agent = Agent(env=mountain_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation settings\n",
    "episode_number = 0\n",
    "number_of_episodes=1500\n",
    "metric = Metric(number_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode_number<number_of_episodes:\n",
    "    \n",
    "    metric.reset()\n",
    "    \n",
    "    R,episode_length= run(agent,mountain_car)\n",
    "    \n",
    "    metric.add(R,episode_number,episode_length)\n",
    " \n",
    "    episode_number += 1\n",
    "    \n",
    "    \n",
    "    if episode_number%100==0:\n",
    "        \n",
    "        metric.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "agent.brain.model.save('Models/dqn.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "\n",
    "plt.plot(metric.G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.mean_G_all)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.episodes_length)\n",
    "plt.ylabel('Episode Length')\n",
    "plt.xlabel('Number of Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Test the agent\n",
    "\n",
    "Now, let's see how much our get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    _,_= run(agent,mountain_car, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Dive deeper into DQN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to implement DQN from scratch!. the section will introduce DQN components first, then we will try to stack these components together, to get a fully DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1- DQN Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN algorithm consists of 3 main components:\n",
    "\n",
    "   1. Function approximator ( Neural Network ) .\n",
    "   2. Reply buffer. ( Memory )\n",
    "   3. Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any machine learning algorithm can be seen as a function approximator, DQN algorithm uses the Neural Network as a function approximator. In the case of DQN the neural network outputs continuous value. \n",
    "\n",
    "- There are many deep learning frameoworks to implement the neural networks. This tutorial is based on Keras. Keras is a high API for other deep learning frameworks, such as Tensorflow, Theano and the CNTK.\n",
    "\n",
    "- DQN uses the neural network to predict the action value in a given state $Q(s,a)$ . \n",
    "The Brain class represents the neural netowrks. It consists of 3 functions:\n",
    "    -  create() : Which create the neural network.\n",
    "    -  train():   train the model.\n",
    "    -  predict(): predict the Q(s,a) value, it takes a state and output Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain Settings:\n",
    "\n",
    "#batch_size=64\n",
    "#hidden_units=64\n",
    "#\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self,state_size, action_size,batch_size, hidden_units,learning_rate):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.params = {}\n",
    "        self.model= self._create()\n",
    "        # self.model.load_weights(\"cartpole-basic.h5\")\n",
    "        \n",
    "    def _create(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(units=self.hidden_units, activation='relu', input_dim= self.state_size))\n",
    "        model.add(layers.Dense(units= self.action_size, activation='linear'))\n",
    "\n",
    "       \n",
    "        W1=model.get_weights()[0]\n",
    "        b1=model.get_weights()[1]\n",
    "        W2=model.get_weights()[2]\n",
    "        b2=model.get_weights()[3]\n",
    "        \n",
    "        self.params = dict(W1=W1, b1=b1, W2=W2, b2=b2)            \n",
    "        \n",
    "       \n",
    "        optimizer = optimizers.adam(lr=self.lr)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=self.batch_size,epochs=epoch, verbose=verbose)\n",
    "        \n",
    "\n",
    "    def predict(self, s):\n",
    "        #print(s.shape)\n",
    "        return self.model.predict(s,batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experience Replay Buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN use a memory to store the episode data $(s_t , a_t , r_t, s_{t+1})$ as a training set. \n",
    "- Experience increase the stability of the learning.\n",
    "- Experience replay averages the behavior distribution over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def get_sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy: a policy is a function that map from state to action, not action value but action. $ \\pi(s):a$\n",
    "- In this tutorial, we implement the policy as a python class. The calss have 2 functions:\n",
    "    - get_action: take an state and return the action.It return a random action with probability $\\epsilon$ and an action based on the the output of the neural network with probability $1-\\epsilon$.\n",
    "    \n",
    "    - decay_epsilon: which encourages the agent to take actions that not depending on its main policy. In the begining $\\epsilon$ value is high which encourages the agent to take random actions, and through the time it gets to decay which let the agent take actions based on its own policy ( the Brain or Neural Network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    epsilon = MAX_EPSILON\n",
    "    \n",
    "    def __init__(self, ACTION_COUNT):\n",
    "        self.ACTION_COUNT = ACTION_COUNT\n",
    "        pass\n",
    "    \n",
    "    def get_action(self,s,brain):\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.ACTION_COUNT-1)\n",
    "        else:\n",
    "            s=np.reshape(s,newshape=(1,s.shape[0]))\n",
    "            return np.argmax(brain.predict(s)) \n",
    "    \n",
    "    def decay_epsilon(self,steps):\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * steps)\n",
    "        return self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  Stack the component together\n",
    "\n",
    "- In the previous section, we introduce DQN main components, in this section we will combine these components together to make DQN algorithm.\n",
    "\n",
    "- To combine these components, we define a class called Agent, which will use all the previous code when it is intracing with environment. \n",
    "\n",
    "- Our Agent consists from 3 functions:\n",
    "    - act(): take a state and return an action. it uses: get_action in policy class.\n",
    "    - observe(): take  $(s_t , a_t , r_t, s_{t+1})$ and save in the memory or the Replay buffer. it uses:\n",
    "        - add sample from Memory class.\n",
    "        - decay_epsilon from Policy class.\n",
    "        \n",
    "    - replay(): update the neural network parameters. It uses:\n",
    "        - get_sample in Memory class.\n",
    "        - predict in Brain class.\n",
    "        - train in Brain classs.\n",
    "        \n",
    "- Because Replay is an important function, this how it works:\n",
    "    - It request data from the Memory, the number of samples from the memory called Batch. Each samples consist of $(s_t, a_t, r_t, s_{t+1})$.\n",
    "   \n",
    "    - Predict the actions values of all the $s_t$.\n",
    "    - Predict the actions values of all the $s_{t+1}$.\n",
    "    - Update the actions values of $s_t$.\n",
    "    - Construct the data to update the Neural Network. data consists of:\n",
    "        - X represents the states.\n",
    "        - y represents the actions values that corsponding to each state. \n",
    "    - Update the Networks with the constructed data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    steps = 0\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.STATE_COUNT  = self.env.observation_space.shape[0]\n",
    "        self.ACTION_COUNT = self.env.action_space.n\n",
    "    \n",
    "        self.brain = Brain(self.STATE_COUNT,self.ACTION_COUNT)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        self.policy = Policy(self.ACTION_COUNT)\n",
    "        \n",
    "    def act(self, s):\n",
    "        action=self.policy.get_action(s,self.brain)\n",
    "        return action\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add_sample(sample)    \n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.policy.decay_epsilon(self.steps)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.get_sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = np.zeros(self.STATE_COUNT)\n",
    "\n",
    "        \n",
    "        states = np.array([ o[0] for o in batch ], dtype=np.float32)\n",
    "        states_ = np.array([(no_state if o[3] is None else o[3]) for o in batch ], dtype=np.float32)\n",
    "\n",
    "        p = self.brain.predict(states)\n",
    "        p_ = self.brain.predict(states_)\n",
    "\n",
    "        x = np.zeros((batchLen, self.STATE_COUNT)).astype(np.float32)\n",
    "        y = np.zeros((batchLen, self.ACTION_COUNT)).astype(np.float32)\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            s, a, r, s_ = batch[i]\n",
    "            \n",
    "            action_values = p[i]\n",
    "            if s_ is None:\n",
    "                action_values[a] = r\n",
    "                \n",
    "            else:\n",
    "                action_values[a] = r + GAMMA * np.amax(p_[i])      # calculate the target: r+ Gamma*max Q(s',a')\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = action_values\n",
    "\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Run the code\n",
    "\n",
    "- Follow these steps to implement the run function below:\n",
    "\n",
    "    - Create an object from the Agent class.\n",
    "    - Loop over the number of iterations.\n",
    "    - Loop over the number of time step.\n",
    "    - Use act() function to apply action in the environment through step() function.\n",
    "    - Step() will returns the episode trajectory, save it in the memory.\n",
    "    - Use replay() function to update the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env):\n",
    "    s = env.reset()\n",
    "    R = 0 \n",
    "\n",
    "    while True:     \n",
    "\n",
    "        a = agent.act(s.astype(np.float32))\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "\n",
    "        if done: # terminal state\n",
    "            s_ = None\n",
    "        \n",
    "        \n",
    "        agent.observe((s, a, r, s_))\n",
    "        agent.replay()            \n",
    "\n",
    "        s = s_\n",
    "        R += r\n",
    "\n",
    "        if done:\n",
    "            env.close()\n",
    "            return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()    # Create an object from the Agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_number = 0\n",
    "number_of_episodes=1000\n",
    "\n",
    "while episode_number<number_of_episodes:\n",
    "    episode_states=[]\n",
    "    episode_rewards=[]\n",
    "    episode_actions=[]\n",
    "    episode_length_counter=1\n",
    "    \n",
    "    R,episode_length_counter= run(agent)\n",
    "    \n",
    "    G.append(R)\n",
    "    reward_sum=sum(G)\n",
    "\n",
    "    episode_number += 1\n",
    "\n",
    "    if episode_number%100==0:\n",
    "                \n",
    "        print(\"==========================================\")\n",
    "        print(\"Episode: \", episode_number)\n",
    "        print(\"Rewards: \", R)\n",
    "        print(\"Max reward so far: \", max(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Big Picture of Reinforcement learing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 General RL taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RL algorithms can be divided to 3 types:\n",
    "    - Value functions based Methods.\n",
    "    - Policy based Methods.\n",
    "    - Hybrid Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Value function Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Policy function Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Hybrid Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Drawback of Value function Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Next Steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python|RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
