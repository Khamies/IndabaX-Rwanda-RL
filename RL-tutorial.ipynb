{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Waleed-Daud/IndabaX-Rwanda-RL/blob/master/RL-tutorial.ipynb\" \n",
    "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "In this practical we introduce the idea of reinforcement learning, discuss how it differs from supervised and unsupervised learning and then build an agent that learns to drive a car up the mountain.\n",
    "\n",
    "# **Learning Objectives**\n",
    "* Understand what is reinforcement learning and how it differs from supervised and unsupervised learning.\n",
    "* Understand the relationship between the **environment** and the **agent**.\n",
    "* Understand what is a **policy** and how a **policy** is used by an agent to select an action.\n",
    "* Understand how the **state**, **action** and **reward** are communicated between the agent and the environment. \n",
    "* Understand **Q-learning** algorithm.\n",
    "* Understand and Implement **DQN** algorithm.\n",
    "* Discover at least one potential issue with the DQN.\n",
    "* See the big picture of Reinforcment Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learning Paradims**\n",
    " \n",
    "#### **1- Supervised learning:**\n",
    "Is a learning paradaim, given an input and a target value or class. **The goal is to predict the target value.**\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"850\" height=\"800\" src=\"data/supervised_learning.gif\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\" > Image by: Venkatesh Tata </a>\n",
    "</p>\n",
    "  \n",
    "#### **2- Unsupervised learning:**\n",
    "Is a learning paradaim, where **we are only given an input and we don't have the output**. **The goal is to look for patterns in that input.**\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"800\" src=\"data/unsupervised.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\" > Image by: Explorez vos données avec des algorithmes non supervisés </a>\n",
    "</p>\n",
    "\n",
    "#### **3- Reinforcement learning:** \n",
    "Is a learning paradaim, which cares about training an **agent** to maximise a **reward** it obtains through interaction with an **environment**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL Overview](data/rl-image.png) \n",
    "\n",
    "<div> <a > Image by: Benjamin Rosman </a> </div>\n",
    "\n",
    "\n",
    "\n",
    "#### **How it works?**\n",
    "\n",
    "The **environment** defines a set of **actions** that an agent can take. The agent observes the current **state** of the environment, tries actions and *learns* a **policy** which is a distribution over the possible actions given a state of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **Table of contents**\n",
    "\n",
    "We will use an OpenAI Gym environment called **Mountain Car**, the goal is to train an agent to drive a car up the mountain. The practical will be as follows:\n",
    "\n",
    "1. Introduce the **Mountain Car** environment and explore the states and actions available.\n",
    "2. Create a simple agent that takes random actions.\n",
    "3. Going from random agent to skilled agent:        \n",
    "4. Merge Q-learning with Deep Learning:\n",
    "5. Dive deeper into DQN:    \n",
    "6. The big picture of Reinforcement learing:\n",
    "7. References.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Waleed-Daud/IndabaX-Rwanda-RL.git\n",
    "% cd IndabaX-Rwanda-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DQN import Agent\n",
    "from DQN import run\n",
    "from DQN import Metric\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras  import optimizers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1- Explore the Environment**\n",
    "\n",
    "![SegmentLocal](data/environment.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Now let us take a look to how the states and actions represented:\n",
    "\n",
    "**States:**\n",
    "\n",
    "\n",
    "|Representation |  State |   Min value|  Max value  |\n",
    "|---|---------------|------|-------|\n",
    "| 0 |  position |  -1.2 |     0.6 | \n",
    "| 1 |   velocity|   -0.07|     0.07 | \n",
    "\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "\n",
    "|Representation |  Action| \n",
    "|---|---------------|\n",
    "| 0 |  push left |\n",
    "| 1 |   no push|  \n",
    "| 2 |    push right|  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car = gym.make('MountainCar-v0')\n",
    "# mountain_car = gym.make('CartPole-v0')\n",
    "\n",
    "num_states = mountain_car.observation_space.shape[0]\n",
    "num_actions = mountain_car.action_space.n\n",
    "\n",
    "print(\"States:  {} Type: Contiuous  Represented as: Vector \".format(num_states,) )\n",
    "print(\"Actions: {} Type: Discrete   Represented as: scalar or number \".format(num_actions+1))  # adding one, because action are starting from 0.\n",
    "print(\"Example of a state: \", mountain_car.observation_space.sample())\n",
    "print(\"Example of an action: \", mountain_car.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **2- Build a random agent**\n",
    "\n",
    "**reset:** get an initial state in the envirnoment.\n",
    "\n",
    "**render:** show the mountain car environemnt (simulator) in the screen.\n",
    "\n",
    "**step:** apply the action in the environemnt.\n",
    "\n",
    "**smaple:** sample an action\n",
    "\n",
    "**close:**  close the envrinment ( simulator ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**( uncomment and run this in your computer, not in colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car.reset()\n",
    "for _ in range(1000):\n",
    "    mountain_car.render()\n",
    "    random_agent = mountain_car.action_space.sample()\n",
    "    mountain_car.step(random_agent) # take a random action\n",
    "mountain_car.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 3- Going from random agent to skilled agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Q-learning Theory**\n",
    "\n",
    "As we mentioned before, the goal of Reinforcement Learning is to train an agent to maximise a reward it obtains through interaction with an environment.\n",
    "\n",
    "#### **So, what is reward that the agent want to maximize?**\n",
    "\n",
    "Basically, it is the objective function of the agent, and has another name called Reward function. It formalized matmatically as:\n",
    "\n",
    "\\begin{align}\n",
    "&& \\large  G_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\gamma^{3} r_{t+3} + .... =  \\sum_{t=1}^{T_i} \\gamma^t r_{i,t}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q-learning**:\n",
    "\n",
    "- Is an **Off-Policy** RL algorithm for **Temporal Difference learning**.\n",
    "- Off-policy: the agent updates the actions values based on a policy that differs from the one that used to interact with the environment.\n",
    "- On-policy: the agent updates the actions values based on the same policy that use it to interact with the environment.\n",
    "- Temporal Difference learning (TD) : a reinforcement learning algorithm that similar to the Q-learning but it is on-policy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q-learning considers that every action in a given state has a value $ Q(s_t,a_t)$, this value formalized matmatically as:**\n",
    "\n",
    "\\begin{align}\n",
    "&& \\large Q_n(s_t,a_t) = Q_{n-1}(s_t,a_t) + \\alpha [r_{t} + \\max_{a} \\gamma Q_n(s_{t+1},a) - Q_{n-1}(s_t,a_t) ] \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2  The intuition behind Q-learning fromula**\n",
    "\n",
    "**The formula actually saying:** the value of the current action depends on its previous value and a constant multilplied by a linear combination of the discounted maximum action value in the next state **and** the previous action value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do these terms mean?**\n",
    "\n",
    "- $ Q_n(s_t,a_t) \\equiv \\space The \\space value \\space of \\space action \\space a_t \\space   \\space in  \\space the  \\space current \\space  time \\space  step \\space s_t. $\n",
    "\n",
    "- $ \\max_{a} \\gamma Q_n(s_{t+1},a) \\equiv \\space The \\space max \\space value \\space over \\space all \\space the \\space actions \\space in \\space the \\space next \\space state \\space s_{t+1}.$\n",
    "\n",
    "- $\\alpha \\equiv \\space Learning \\space rate $\n",
    "\n",
    "- $r_t  \\equiv current \\space reward \\space at \\space time \\space step \\space t .$\n",
    "\n",
    "- $\\gamma \\equiv Discount \\space factor.$ \n",
    "\n",
    "- $[ r_t +  \\max_{a} \\gamma Q_n(s_{t+1},a)] \\equiv \\space TD \\space target $\n",
    "\n",
    "- $ [r_{t} + \\max_{a} \\gamma Q_n(s_{t+1},a) - Q_{n-1}(s_t,a_t) ] \\equiv  TD \\space error.$\n",
    "\n",
    "\n",
    "**Why the discount factor $\\gamma$ ?**\n",
    "\n",
    "That means, we do care about the current rewards more than long term rewards, the highest discount factor the more foucsing on the short rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Q-learning algorithm:**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"800\" src=\"data/Q-learning.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Ways to implement Q-learning**\n",
    "\n",
    "1. You can implement Q-learning using:\n",
    "\n",
    "    - Q-Table ( Dictionary, Matrix ,.... ) : Each cell in the table represents action value in a given state.\n",
    "             \n",
    "|   |  |  Q-Table |    |\n",
    "|---|---------------|------|-------|---\n",
    "| $s_0$ |  $a_{00}$|  $a_{01}$|     $a_{02}$ | $a_{03}$ | ... |...$a_{0n}$...\n",
    "| $s_1$ |    $a_{10}$|  $a_{11}$|     $a_{12}$ | $a_{13}$ | ... |...$a_{1n}$...\n",
    "| $s_2$ |   $a_{20}$|  $a_{21}$|     $a_{22}$ | $a_{23}$ | ... |...$a_{2n}$...\n",
    "| $s_3$|   $a_{30}$|  $a_{31}$|     $a_{32}$ | $a_{33}$ | ... |...$a_{3n}$...\n",
    "| .. |  ..|  ..|      .. | .. | ... |...\n",
    "        \n",
    "   \n",
    "   \n",
    "\n",
    "   - Function Approximation: Because of the increasing of the size of Q-table with number of states, we approximate the action     value using **a function approximator like Neural Network**.\n",
    "\n",
    "![Function approximator](data/func.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **4- Merge Q-learning with Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DQN:**\n",
    "\n",
    "- Stands for **Deep Q-learning Network**. \n",
    "- A deep learning model to successfully learn control policies directly from high dimensional sensory input using Q-learning algorithm based method.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Train the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent\n",
    "agent = Agent(env=mountain_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation settings\n",
    "episode_number = 0\n",
    "max_episodes=1000\n",
    "metric = Metric(max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode_number<max_episodes:\n",
    "    \n",
    "    metric.reset()\n",
    "    \n",
    "    R,episode_length= run(agent,mountain_car)\n",
    "    \n",
    "    metric.add(R,episode_number,episode_length)\n",
    " \n",
    "    episode_number += 1\n",
    "    \n",
    "    \n",
    "    if episode_number%100==0:\n",
    "        \n",
    "        metric.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "agent.brain.model.save('Models/dqn.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "\n",
    "plt.plot(metric.G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.mean_G_all)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.episodes_length)\n",
    "plt.ylabel('Episode Length')\n",
    "plt.xlabel('Number of Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Test the agent**\n",
    "\n",
    "Now, let's see how much our get better. **( uncomment and run this in your computer, not in colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     _,_= run(agent,mountain_car, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **5- Dive deeper into DQN: (Optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to implement DQN from scratch!. the section will introduce DQN components first, then we will try to stack these components together, to get a fully DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1- DQN Components:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN algorithm consists of 3 main components:**\n",
    "\n",
    "   1. Function approximator ( Neural Network ) .\n",
    "   2. Reply buffer. ( Memory )\n",
    "   3. Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"800\" src=\"data/DQN.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Function approximator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any machine learning algorithm can be seen as a function approximator, DQN algorithm uses the Neural Network as a function approximator. In the case of DQN the neural network outputs continuous value. \n",
    "\n",
    "- There are many deep learning frameoworks to implement the neural networks. This tutorial is based on Keras. Keras is a high API for other deep learning frameworks, such as Tensorflow, Theano and the CNTK.\n",
    "\n",
    "- DQN uses the neural network to predict the action value in a given state $Q(s,a)$ . \n",
    "The Brain class represents the neural netowrks. It consists of 3 functions:\n",
    "    -  create() : Which create the neural network.\n",
    "    -  train():   train the model.\n",
    "    -  predict(): predict the Q(s,a) value, it takes a state and output Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self,state_size, action_size,batch_size,learning_rate):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.params = {}\n",
    "        self.model= self._create()\n",
    "       \n",
    "        \n",
    "    def _create(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(units=64, activation='relu', input_dim= self.state_size))\n",
    "        model.add(layers.Dense(units= self.action_size, activation='linear'))        \n",
    "        \n",
    "       \n",
    "        optimizer = optimizers.adam(lr=self.lr)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=self.batch_size,epochs=epoch, verbose=verbose)\n",
    "        \n",
    "\n",
    "    def predict(self, s):\n",
    "        \n",
    "        return self.model.predict(s,batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Experience Replay Buffer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN use a memory to store the episode data **(transition tuple)** $(s_t , a_t , r_t, s_{t+1})$ as a training set. \n",
    "- Experience increase the stability of the learning.\n",
    "- alleviates the problems of correlated data and non-stationary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def get_sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Policy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy: a policy is a function that map from state to action, not action value but action. $ \\pi(s):a$\n",
    "- In this tutorial, we implement the policy as a python class. The calss have 2 functions:\n",
    "    - get_action: take an state and return the action.It return a random action with probability $\\epsilon$ and an action based on the the output of the neural network with probability $1-\\epsilon$.\n",
    "    \n",
    "    - decay_epsilon: which encourages the agent to take actions that not depending on its main policy. In the begining $\\epsilon$ value is high which encourages the agent to take random actions, and through the time it gets to decay which let the agent take actions based on its own policy ( the Brain or Neural Network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    def __init__(self, ACTION_COUNT):\n",
    "        self.ACTION_COUNT = ACTION_COUNT\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon = self.max_epsilon\n",
    "        self.lmbda = 0.0001\n",
    "        self.steps = 0\n",
    "        pass\n",
    "    \n",
    "    def get_action(self,s,brain):\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.ACTION_COUNT-1)\n",
    "        else:\n",
    "            s=np.reshape(s,newshape=(1,s.shape[0]))\n",
    "            return np.argmax(brain.predict(s)) \n",
    "    \n",
    "    def decay_epsilon(self,steps):\n",
    "        self.epsilon = self.max_epsilon + (self.max_epsilon - self.min_epsilon) * math.exp(-self.lmbda * self.steps)\n",
    "        return self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2  Stack the component together**\n",
    "\n",
    "- In the previous section, we introduce DQN main components, in this section we will combine these components together to make DQN algorithm.\n",
    "\n",
    "- To combine these components, we define a class called Agent, which will use all the previous code when it is intracing with environment. \n",
    "\n",
    "- Our Agent consists from 3 functions:\n",
    "    - act(): take a state and return an action. it uses: get_action in policy class.\n",
    "    - observe(): take  $(s_t , a_t , r_t, s_{t+1})$ and save in the memory or the Replay buffer. it uses:\n",
    "        - add sample from Memory class.\n",
    "        - decay_epsilon from Policy class.\n",
    "        \n",
    "    - replay(): update the neural network parameters. It uses:\n",
    "        - get_sample in Memory class.\n",
    "        - predict in Brain class.\n",
    "        - train in Brain classs.\n",
    "        \n",
    "- Because Replay is an important function, this how it works:\n",
    "    - It request data from the Memory, the number of samples from the memory called Batch. Each samples consist of $(s_t, a_t, r_t, s_{t+1})$.\n",
    "   \n",
    "    - Predict the actions values of all the $s_t$.\n",
    "    - Predict the actions values of all the $s_{t+1}$.\n",
    "    - Update the actions values of $s_t$.\n",
    "    - Construct the data to update the Neural Network. data consists of:\n",
    "        - X represents the states.\n",
    "        - y represents the actions values that corsponding to each state. \n",
    "    - Update the Networks with the constructed data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    steps = 0\n",
    "    MEMORY_CAPACITY = 100000\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.99\n",
    "    lEARNING_RATE = 0.001\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.STATE_COUNT  = self.env.observation_space.shape[0]\n",
    "        self.ACTION_COUNT = self.env.action_space.n\n",
    "        self.memory_capacity = 100000\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "        self.brain = Brain(self.STATE_COUNT,self.ACTION_COUNT,self.batch_size,self.lr)\n",
    "        self.memory = Memory(self.memory_capacity)\n",
    "        self.policy = Policy(self.ACTION_COUNT)\n",
    "        \n",
    "    def act(self, s):\n",
    "        action=self.policy.get_action(s,self.brain)\n",
    "        return action\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add_sample(sample)    \n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.policy.decay_epsilon(self.steps)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.get_sample(self.batch_size)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = np.zeros(self.STATE_COUNT)\n",
    "\n",
    "        \n",
    "        states = np.array([ o[0] for o in batch ], dtype=np.float32)\n",
    "        states_ = np.array([(no_state if o[3] is None else o[3]) for o in batch ], dtype=np.float32)\n",
    "\n",
    "        p = self.brain.predict(states)\n",
    "        p_ = self.brain.predict(states_)\n",
    "\n",
    "        x = np.zeros((batchLen, self.STATE_COUNT)).astype(np.float32)\n",
    "        y = np.zeros((batchLen, self.ACTION_COUNT)).astype(np.float32)\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            s, a, r, s_ = batch[i]\n",
    "            \n",
    "            action_values = p[i]\n",
    "            if s_ is None:\n",
    "                action_values[a] = r\n",
    "                \n",
    "            else:\n",
    "                action_values[a] = r + self.gamma * np.amax(p_[i])      # calculate the target: r+ Gamma*max Q(s',a')\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = action_values\n",
    "\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 Run the code**\n",
    "\n",
    "- Follow these steps to implement the run function below:\n",
    "\n",
    "    - Create an object from the Agent class.\n",
    "    - Loop over the number of iterations.\n",
    "    - Loop over the number of time step.\n",
    "    - Use act() function to apply action in the environment through step() function.\n",
    "    - Step() will returns the episode trajectory, save it in the memory.\n",
    "    - Use replay() function to update the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sc = Agent(env=mountain_car)\n",
    "max_episodes=1000\n",
    "metric_sc = Metric(max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent, env):\n",
    "    s = env.reset()\n",
    "    R = 0 \n",
    "    episode_length_counter=1\n",
    "\n",
    "    while True:     \n",
    "        a = agent.act(s.astype(np.float32))\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        episode_length_counter+=1\n",
    "        \n",
    "        agent.observe((s, a, r, s_))\n",
    "        agent.replay()            \n",
    "        \n",
    "        if done: # terminal state\n",
    "            s_ = None\n",
    "            env.close()\n",
    "            return R,episode_length_counter\n",
    "        \n",
    "        s = s_\n",
    "        R += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_number = 0\n",
    "\n",
    "while episode_number<max_episodes:\n",
    "    metric_sc.reset()\n",
    "    \n",
    "    R,episode_length = run_agent(agent_sc, mountain_car)\n",
    "    \n",
    "    metric_sc.add(R,episode_number,episode_length)\n",
    "\n",
    "    episode_number += 1\n",
    "\n",
    "    if episode_number%100==0:\n",
    "        \n",
    "        metric_sc.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **6. The Big Picture of Reinforcement learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1 General RL taxonomy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RL algorithms can be divided to 3 types:\n",
    "    - Value functions based Methods.\n",
    "    - Policy based Methods.\n",
    "    - Hybrid Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A) Value function Methods:**\n",
    "\n",
    "- A value functions methods consider that every state or action has a value that can be learned, and then taking decisions based on those values. ( i.e taking actions based on the values of the actions) .\n",
    "\n",
    "- As an example to value function methods: SARSA, Q-learning, DQN algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B) Policy function Methods:**\n",
    "\n",
    "- Policy function methods don't need to learn values for actions or states to take decisions.\n",
    "- The inputs are the state features and the ouputs are the actions or state . $\\pi(s): a$\n",
    "- They more stable more than Value function mthods, and can scale up in high dimensional state space and action space.\n",
    "- An example to policy function methods: REINFORCE, DDPG algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C) Hybrid Methods:**\n",
    "\n",
    "- These methods use both value function and policy function methods.\n",
    "- It makes the learning even more stable than Policy iteration.\n",
    "- Typically, two neural networks are used, one as a value function approximator and the other as policy function approximator.\n",
    "- It needs more time time to be trained than the policy iteration.\n",
    "- Example to some of these methods, ACTOR Critic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 Drawback of Value function Methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They can't be used in problems that have high dimension action space, because the number of the neural network outputs are increasing with the number of the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **7- References**\n",
    "\n",
    " - [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) [Book]\n",
    " - [Deep Reinforcement Learning by Sergey Levine](http://rail.eecs.berkeley.edu/deeprlcourse/). [Course]\n",
    " - [Introductioon to Reinforcement learning by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html). [Course]\n",
    " - [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) [DQN paper]\n",
    " - [Simple Reinforcement Learning with Tensorflow](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0) [blog]\n",
    " - [ An introduction to Reinforcement Learning](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419) [blog]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# **8- Next Steps.**\n",
    "\n",
    "- To see the current research problems in RL check this: [Deep RL does not work yet](https://www.alexirpan.com/2018/02/14/rl-hard.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python|RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
