{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this practical we introduce the idea of reinforcement learning, discuss how it differs from supervised and unsupervised learning and then build an agent that learns to drive a car up the mountain.\n",
    "\n",
    "## Learning Objectives\n",
    "* Understand the relationship between the **environment** and the  **agent**  \n",
    "* Understand how a **policy** is used by an agent to select an action\n",
    "* Describe how to implement a **run-loop** that controls the interaction between environement and agent.\n",
    "* Understand how the **state**, **action** and **reward** are communicated between the agent and the environment.  \n",
    "* Be able to implement the a simple **Q-learning** RL algorithm call **DQN**\n",
    "* Discover at least one potential issue with the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## A) Learning Paradims\n",
    "**Supervised learning: ** is a learning paradaim, given an input and a target value or class. The goal is to predict the class value.\n",
    "\n",
    "**Unsupervised learning: ** is a learning paradaim, where we are only given an input. The goal is to look for patterns in that input. \n",
    "\n",
    "**Reinforcement learning: ** which cares about training an **agent** to maximise a **reward** it obtains through interaction with an **environment**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Reinforcement Learning ( RL )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How RL works?\n",
    "\n",
    "The **environment** defines a set of **actions** that an agent can take. The agent observes the current **state** of the environment, tries actions and *learns* a **policy** which is a distribution over the possible actions given a state of the environment. \n",
    "\n",
    "The following diagram illustrates the interaction between the agent and environment. We will explore each of the terms in more detail throughout this practical. \n",
    "\n",
    "\n",
    "![RL Overview](data/rl-image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Outlines\n",
    "\n",
    "We will use an OpenAI Gym environment called **Mountain Car**, the goal is to train an agent to drive a car up the mountain. The practical will be as follows:\n",
    "\n",
    "1. Introduce the **Mountain Car** environment and explore the states and actions available.\n",
    "2. Create a simple agent that takes random actions.\n",
    "3. Going from random agent to skilled agent:\n",
    "    - Introduce Q-learning algorithm.\n",
    "    - The intuition behind Q-learning.\n",
    "    - Ways to implement Q-learning:\n",
    "        - Tabular Method\n",
    "        - Function approximator: Neural Network.\n",
    "        \n",
    "4. Merge Q-learning with Deep Learning:\n",
    "    - Introduce DQN algorithm.\n",
    "    - Use DQN!!\n",
    "    - Evaluate RL Algorithm:\n",
    "        -  Compare DQN with the random agent.\n",
    "5. Going deeper into DQN:\n",
    "    - DQN main momponents\n",
    "    - Implement the components.\n",
    "    - Stack the components together.\n",
    "    \n",
    "6. The big picture of Reinforcement learing:\n",
    "    - General RL taxonomy\n",
    "        - Value function based method.\n",
    "        - Gradient based Methods.\n",
    "        - Hybrid based Methods.\n",
    "    - Drawback of the Value function Methods.\n",
    "    - Next Steps.\n",
    "\n",
    "7. References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Explore the Environment\n",
    "\n",
    "![SegmentLocal](data/environment.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Now let us take a look to how the states and actions represented:\n",
    "\n",
    "**States:**\n",
    "\n",
    "\n",
    "|Representation |  State |   Min value|  Max value  |\n",
    "|---|---------------|------|-------|\n",
    "| 0 |  position |  -1.2 |     0.6 | \n",
    "| 1 |   velocity|   -0.07|     0.07 | \n",
    "\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "\n",
    "|Representation |  Action| \n",
    "|---|---------------|\n",
    "| 0 |  push left |\n",
    "| 1 |   no push|  \n",
    "| 2 |    push right|  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car = gym.make('MountainCar-v0')\n",
    "# mountain_car = gym.make('CartPole-v0')\n",
    "\n",
    "num_states = mountain_car.observation_space.shape[0]\n",
    "num_actions = mountain_car.action_space.n\n",
    "\n",
    "print(\"States:  {} Type: Contiuous  Represented as: Vector \".format(num_states,) )\n",
    "print(\"Actions: {} Type: Discrete   Represented as: scalar or number \".format(num_actions+1))  # adding one, because action are starting from 0.\n",
    "print(\"Example of a state: \", mountain_car.observation_space.sample())\n",
    "print(\"Example of an action: \", mountain_car.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Build a random agent\n",
    "\n",
    "**reset:** get an initial state in the envirnoment.\n",
    "\n",
    "**render:** show the mountain car environemnt (simulator) in the screen.\n",
    "\n",
    "**step:** apply the action in the environemnt.\n",
    "\n",
    "**smaple:** sample an action\n",
    "\n",
    "**close:**  close the envrinment ( simulator ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car.reset()\n",
    "for _ in range(1000):\n",
    "    mountain_car.render()\n",
    "    random_agent = mountain_car.action_space.sample()\n",
    "    mountain_car.step(random_agent) # take a random action\n",
    "mountain_car.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Going from random agent to skilled agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Q-learning\n",
    "\n",
    "As we mentioned before, the goal of Reinforcement learning is to train an agent to maximise a reward it obtains through interaction with an environment.\n",
    "\n",
    "So, what is reward that the agent want to maximize?\n",
    "\n",
    "Basically, it is the objective function of the agent, and has another name called Reward function. It formalized matmatically as:\n",
    "\n",
    "$ G_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\gamma^{3} r_{t+3} + .... =  \\sum_{t=1}^{T_i} \\gamma^t r_{i,t}$\n",
    "\n",
    "Q-learning considers that every action in a given state has a value $ Q(s_t,a_t)$, this value formalized matmatically as:\n",
    "\n",
    "\n",
    "$ Q(s_t,a_t) = Q(s_t,a_t) + \\alpha [r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] $\n",
    "\n",
    "which is equal to : $ E[G_t|\\space s_t ,a_t]$\n",
    "\n",
    "Looking for a proof for the previous formula? go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  intuition behind Q-learning.\n",
    "\n",
    "Now, Let us get the intiution behind this formula:\n",
    "\n",
    "\n",
    "$ Q(s_t,a_t) = Q(s_t,a_t) + \\alpha [r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] $\n",
    "\n",
    "$Q(s_t,a_t) \\equiv Action \\space value \\space a_t \\space at \\space state \\space s_t \\\\\n",
    "\\max_{a} \\gamma Q(s_{t+1},a) \\equiv max \\space action \\space value \\space over \\space all \\space the \\space actions \\space in \\space the \\space next \\space state \\space s_{t+1} \\equiv is \\space called \\space TD \\space target \\\\ \n",
    "[r_{t} + \\max_{a} \\gamma Q(s_{t+1},a) - Q(s_t,a_t) ] \\equiv is \\space called \\space TD \\space error.\n",
    "$\n",
    "\n",
    "The formula actually saying: the value of the action depends on its previous value plus the discount rewards since taking that action and onward.\n",
    "This the discount rewards representes TD target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ways to implement Q-learning\n",
    "\n",
    "1. You can implement Q-learning using:\n",
    "\n",
    "    - Q-Table ( Dictionary, Matrix ,.... ) : Each cell in the table represents action value in a given state.\n",
    "    - Function Approximation: Because of the increasing of the Q-table with number of states. approximate the action value.\n",
    "             \n",
    "|   |  |  Q-Table |    |\n",
    "|---|---------------|------|-------|---\n",
    "| $s_0$ |  $a_{00}$|  $a_{01}$|     $a_{02}$ | $a_{03}$ | ... |...$a_{0n}$...\n",
    "| $s_1$ |    $a_{10}$|  $a_{11}$|     $a_{12}$ | $a_{13}$ | ... |...$a_{1n}$...\n",
    "| $s_2$ |   $a_{20}$|  $a_{21}$|     $a_{22}$ | $a_{23}$ | ... |...$a_{2n}$...\n",
    "| $s_3$|   $a_{30}$|  $a_{31}$|     $a_{32}$ | $a_{33}$ | ... |...$a_{3n}$...\n",
    "| .. |  ..|  ..|      .. | .. | ... |...\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Merge Q-learning with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DQN import Agent\n",
    "from DQN import run\n",
    "from DQN import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent\n",
    "\n",
    "agent = Agent(env=mountain_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation settings\n",
    "episode_number = 0\n",
    "number_of_episodes=400\n",
    "metric = Metric(number_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode_number<number_of_episodes:\n",
    "    \n",
    "    metric.reset()\n",
    "    \n",
    "    R,episode_length= run(agent,mountain_car)\n",
    "    \n",
    "    metric.add(R,episode_number,episode_length)\n",
    " \n",
    "    episode_number += 1\n",
    "    \n",
    "    \n",
    "    if episode_number%100==0:\n",
    "        \n",
    "        metric.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "agent.brain.model.save('Models/dqn.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "\n",
    "plt.plot(metric.G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.mean_G_all)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metric.episodes_length)\n",
    "plt.ylabel('Episode Length')\n",
    "plt.xlabel('Number of Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Going deeper into DQN:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python|RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
